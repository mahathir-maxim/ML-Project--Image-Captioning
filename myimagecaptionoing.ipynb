{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myimagecaptionoing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GZhgkym5eYO",
        "outputId": "fe9f5e00-677d-4aa7-b79c-5306051f080e"
      },
      "source": [
        "import keras\n",
        "import tensorflow\n",
        "print('keras: %s' % keras.__version__)\n",
        "print('tensorflow: %s' % tensorflow.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keras: 2.7.0\n",
            "tensorflow: 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP4CdjzvJtLI"
      },
      "source": [
        "# Import Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGX8q3CQ6rxS"
      },
      "source": [
        "import numpy as np  \n",
        "# data processing, CSV file I / O (e.g. pd.read_csv)\n",
        "import pandas as pd  \n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\n",
        "from keras.layers import concatenate, BatchNormalization, Input\n",
        "from keras.layers.merge import add\n",
        "from  tensorflow.keras.utils import to_categorical, plot_model\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "import matplotlib.pyplot as plt  # for plotting data\n",
        "import cv2\n",
        "import urllib.request\n",
        "import string\n",
        "import glob\n",
        "from keras.preprocessing.image import load_img, img_to_array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt3s_c-IJyv_"
      },
      "source": [
        "# Loading Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-afVaX7wbE8n"
      },
      "source": [
        "def load_description(text):\n",
        "    mapping = dict()\n",
        "    for line in text:\n",
        "        \n",
        "        line.split(\"\\n\")\n",
        "        token = line.split(\"\\t\")\n",
        "        if len(line) < 2:   # remove short descriptions\n",
        "            continue\n",
        "        img_id = token[0].split('.')[0]        # name of the image\n",
        "        img_des = token[1].strip()             # description of the image\n",
        "        if img_id not in mapping:\n",
        "            mapping[img_id] = list()\n",
        "        mapping[img_id].append(img_des)\n",
        "    return mapping\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A42dlI53J2Nf"
      },
      "source": [
        "# Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbXvj2X1hQYn"
      },
      "source": [
        "def clean_description(desc):\n",
        "    for key, des_list in desc.items():\n",
        "        for i in range(len(des_list)):\n",
        "            caption = des_list[i]\n",
        "            caption = [ch for ch in caption if ch not in string.punctuation]\n",
        "            caption = ''.join(caption)\n",
        "            caption = caption.split(' ')\n",
        "            caption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]\n",
        "            caption = ' '.join(caption)\n",
        "            des_list[i] = caption\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbWWCiTwJ5C4"
      },
      "source": [
        "# Generating Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxUGujqXiqvU"
      },
      "source": [
        "def to_vocab(desc):\n",
        "    words = set()\n",
        "    for key in desc.keys():\n",
        "        for line in desc[key]:\n",
        "            words.update(line.split())\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7-O2-SGKAig"
      },
      "source": [
        "# Image Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1l0dtsAkII4"
      },
      "source": [
        "def load_clean_descriptions(des, dataset):\n",
        "    dataset_des = dict()\n",
        "    for key, des_list in des.items():\n",
        "        if key+'.jpg' in dataset:\n",
        "            if key not in dataset_des:\n",
        "                dataset_des[key] = list()\n",
        "            for line in des_list:\n",
        "                desc = 'startseq ' + line + ' endseq'\n",
        "                dataset_des[key].append(desc)\n",
        "    return dataset_des"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQWsKI-8KEeQ"
      },
      "source": [
        "# Feature Extracting - added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dptHyr5iKGRJ"
      },
      "source": [
        "def preprocess_img(img_path):\n",
        "    # inception v3 excepts img in 299 * 299 * 3\n",
        "    img = load_img(img_path, target_size = (299, 299))\n",
        "    x = img_to_array(img)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis = 0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "def encode(image):\n",
        "    image = preprocess_img(image)\n",
        "    vec = model.predict(image)\n",
        "    vec = np.reshape(vec, (vec.shape[1]))\n",
        "    return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGqFwrJ5KGe4"
      },
      "source": [
        "# Vocabulary Tokenizing - added"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdPhNIk4KLhh"
      },
      "source": [
        "# Glove Vector Embedding - added"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6SLWSyLKQDq"
      },
      "source": [
        "# Model Definition - added"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Icl-0FaKSLb"
      },
      "source": [
        "# Model Training - added"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9zNYqL8KULM"
      },
      "source": [
        "# Output Prediction - added"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7GedpACKaeS"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "1n1_S6CZhtmt",
        "outputId": "f22d0b38-29c3-435e-da10-806bb897c2a8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    token_path = 'https://raw.githubusercontent.com/mahathir-maxim/ML-Project--Image-Captioning/main/Dataset/Flickr8k_text/Flickr8k.token.txt'\n",
        "    #text = open(token_path, 'r', encoding = 'utf-8').read()\n",
        "    ########\n",
        "    tweetList = []\n",
        "    rawData = urllib.request.urlopen(token_path)\n",
        "    dataLines = rawData.readlines()\n",
        "\n",
        "    for line in dataLines:\n",
        "        decodedLine = line.decode(\"utf-8\")\n",
        "        tweetList.append(decodedLine)\n",
        "    ########\n",
        "    #print(tweetList[0])\n",
        "    descriptions = load_description(tweetList)\n",
        "    #print(descriptions['1000268201_693b08cb0e'])\n",
        "    clean_description(descriptions)\n",
        "    #print(descriptions['1000268201_693b08cb0e'])\n",
        "    vocab = to_vocab(descriptions)\n",
        "    \n",
        "    images = 'https://github.com/mahathir-maxim/ML-Project--Image-Captioning/tree/main/Dataset/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
        "    # Create a list of all image names in the directory\n",
        "    img = glob.glob(images + '*.jpg')\n",
        "      \n",
        "    train_path = 'https://github.com/mahathir-maxim/ML-Project--Image-Captioning/blob/main/Dataset/Flickr8k_text/Flickr_8k.trainImages.txt?raw=true'\n",
        "    #train_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\n",
        "    train_images=[]\n",
        "    rawData = urllib.request.urlopen(train_path)\n",
        "    dataLines = rawData.readlines()\n",
        "    for line in dataLines:\n",
        "        decodedLine = line.decode(\"utf-8\")\n",
        "        train_images.append(decodedLine.strip())\n",
        "    print(train_images[0])\n",
        "\n",
        "    train_descriptions = load_clean_descriptions(descriptions, train_images)\n",
        "    print(train_descriptions['1000268201_693b08cb0e'])\n",
        "\n",
        "    train_img = []  # list of all images in training set\n",
        "    for im in img:\n",
        "        if(im[len(images):] in train_images):\n",
        "            train_img.append(im)\n",
        "\n",
        "    base_model = InceptionV3(weights = 'imagenet')\n",
        "    model = Model(base_model.input, base_model.layers[-2].output)\n",
        "    # run the encode function on all train images and store the feature vectors in a list\n",
        "    encoding_train = {}\n",
        "    for img in train_img:\n",
        "        encoding_train[img[len(images):]] = encode(img)\n",
        "\n",
        "    \n",
        "    # list of all training captions\n",
        "    all_train_captions = []\n",
        "    for key, val in train_descriptions.items():\n",
        "        for caption in val:\n",
        "            all_train_captions.append(caption)\n",
        "      \n",
        "    # consider only words which occur atleast 10 times\n",
        "    vocabulary = vocab\n",
        "    threshold = 10 # you can change this value according to your need\n",
        "    word_counts = {}\n",
        "    for cap in all_train_captions:\n",
        "        for word in cap.split(' '):\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "      \n",
        "    vocab = [word for word in word_counts if word_counts[word] >= threshold]\n",
        "      \n",
        "    # word mapping to integers\n",
        "    ixtoword = {}\n",
        "    wordtoix = {}\n",
        "      \n",
        "    ix = 1\n",
        "    for word in vocab:\n",
        "        wordtoix[word] = ix\n",
        "        ixtoword[ix] = word\n",
        "        ix += 1\n",
        "          \n",
        "    # find the maximum length of a description in a dataset\n",
        "    max_length = max(len(des.split()) for des in all_train_captions)\n",
        "    max_length\n",
        "\n",
        "\n",
        "\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    for key, des_list in train_descriptions.items():\n",
        "        pic = train_images[key + '.jpg']\n",
        "        for cap in des_list:\n",
        "            seq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix]\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n",
        "                out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
        "                # store\n",
        "                X1.append(pic)\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "      \n",
        "    X2 = np.array(X2)\n",
        "    X1 = np.array(X1)\n",
        "    y = np.array(y)\n",
        "      \n",
        "    # load glove vectors for embedding layer\n",
        "    embeddings_index = {}\n",
        "    golve_path ='/kaggle / input / glove-global-vectors-for-word-representation / glove.6B.200d.txt'\n",
        "    glove = open(golve_path, 'r', encoding = 'utf-8').read()\n",
        "    for line in glove.split(\"\\n\"):\n",
        "        values = line.split(\" \")\n",
        "        word = values[0]\n",
        "        indices = np.asarray(values[1: ], dtype = 'float32')\n",
        "        embeddings_index[word] = indices\n",
        "      \n",
        "    emb_dim = 200\n",
        "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
        "    for word, i in wordtoix.items():\n",
        "        emb_vec = embeddings_index.get(word)\n",
        "        if emb_vec is not None:\n",
        "            emb_matrix[i] = emb_vec\n",
        "    emb_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2513260012_03d33305cf.jpg\n",
            "['startseq child in pink dress is climbing up set of stairs in an entry way endseq', 'startseq girl going into wooden building endseq', 'startseq little girl climbing into wooden playhouse endseq', 'startseq little girl climbing the stairs to her playhouse endseq', 'startseq little girl in pink dress going into wooden cabin endseq']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-7f09081bba5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdes_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwordtoix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordtoix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTI6kL9Mi04O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}